
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Gibbs Sampling &#8212; Gaussian Mixture Models</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Experiments" href="../EM/Experiments.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Gaussian Mixture Models</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Gaussian Mixture Models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../metrics.html">
   Metrics
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Expectation-Maximization (EM)
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../EM/theory.html">
   Theory - EM
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../EM/Multivariate/playground-3d.html">
   GMM - Multivariate - Playground
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../EM/Experiments.html">
   Experiments
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Gibbs Sampling
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Gibbs Sampling
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Gibbs-Sampling/theory.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/TheReconPilot/gaussian-mixture-models"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/TheReconPilot/gaussian-mixture-models/issues/new?title=Issue%20on%20page%20%2FGibbs-Sampling/theory.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivation">
   Motivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#background">
   Background
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#monte-carlo">
     Monte Carlo
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#monte-carlo-markov-chain">
     Monte Carlo Markov Chain
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   Gibbs Sampling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-gibbs-sampler-for-unknown-mu-and-sigma">
     Example: Gibbs Sampler for unknown
     <span class="math notranslate nohighlight">
      \(\mu\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(\sigma\)
     </span>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#full-conditional-for-mu">
       Full Conditional for
       <span class="math notranslate nohighlight">
        \(\mu\)
       </span>
       :
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#full-conditional-for-sigma-2">
       Full Conditional for
       <span class="math notranslate nohighlight">
        \(\sigma^2\)
       </span>
       :
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#full-conditional-for-pi">
         Full conditional for
         <span class="math notranslate nohighlight">
          \(\pi\)
         </span>
         :
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Gibbs Sampling</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivation">
   Motivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#background">
   Background
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#monte-carlo">
     Monte Carlo
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#monte-carlo-markov-chain">
     Monte Carlo Markov Chain
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   Gibbs Sampling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-gibbs-sampler-for-unknown-mu-and-sigma">
     Example: Gibbs Sampler for unknown
     <span class="math notranslate nohighlight">
      \(\mu\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(\sigma\)
     </span>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#full-conditional-for-mu">
       Full Conditional for
       <span class="math notranslate nohighlight">
        \(\mu\)
       </span>
       :
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#full-conditional-for-sigma-2">
       Full Conditional for
       <span class="math notranslate nohighlight">
        \(\sigma^2\)
       </span>
       :
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#full-conditional-for-pi">
         Full conditional for
         <span class="math notranslate nohighlight">
          \(\pi\)
         </span>
         :
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="gibbs-sampling">
<h1>Gibbs Sampling<a class="headerlink" href="#gibbs-sampling" title="Permalink to this headline">¶</a></h1>
<div class="section" id="motivation">
<h2>Motivation<a class="headerlink" href="#motivation" title="Permalink to this headline">¶</a></h2>
<p>Calculating a quantity from a probabilistic model is referred to more generally as probabilistic inference, or simply inference.</p>
<p>For example, we may be interested in calculating an expected probability, estimating the density, or other properties of the probability distribution. This is the goal of the probabilistic model, and the name of the inference performed often takes on the name of the probabilistic model, e.g. Bayesian Inference is performed with a Bayesian probabilistic model.</p>
<p>The direct calculation of the desired quantity from a model of interest is intractable for all but the most trivial probabilistic models. Instead, the expected probability or density must be approximated by other means.</p>
<blockquote>
<div><p>For most probabilistic models of practical interest, exact inference is intractable, and so we have to resort to some form of approximation.</p>
</div></blockquote>
<ul class="simple">
<li><p>Pattern Recognition and Machine Learning, 2006 <span id="id1">[<a class="reference internal" href="../intro.html#id3" title="Christopher Bishop. Pattern Recognition and Machine Learning. Springer, January 2006. URL: https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/.">Bis06</a>]</span></p></li>
</ul>
</div>
<div class="section" id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h2>
<div class="section" id="monte-carlo">
<h3>Monte Carlo<a class="headerlink" href="#monte-carlo" title="Permalink to this headline">¶</a></h3>
<p>The solution to the aboce problem is to draw independent samples from the probability distribution, then repeat this process many times to approximate the desired quantity. This is called Monte Carlo sampling.</p>
<p>The problem with Monte Carlo sampling is that it does not work well in high-dimensions:-</p>
<ol class="simple">
<li><p>The curse of dimensionality, where the volume of the sample space increases exponentially with the number of parameters (dimensions).</p></li>
<li><p>This is because Monte Carlo sampling assumes that each random sample drawn from the target distribution is independent and can be independently drawn. This is typically not the case or intractable for inference with Bayesian structured or graphical probabilistic models.</p></li>
</ol>
</div>
<div class="section" id="monte-carlo-markov-chain">
<h3>Monte Carlo Markov Chain<a class="headerlink" href="#monte-carlo-markov-chain" title="Permalink to this headline">¶</a></h3>
<p>To deal with the above drawbacks of Monte Carlo methods, Markov Chain Monte Carlo (MCMC) is used for performing inference for probability distributions where independent samples from the distribution cannot be drawn, or cannot be drawn easily.</p>
<p>Samples are drawn from the probability distribution by constructing a Markov Chain, where the next sample that is drawn from the probability distribution is dependent upon the last sample that was drawn. The idea is that the chain will settle on (find steady state) on the desired quantity we are inferring.</p>
</div>
</div>
<div class="section" id="id2">
<h2>Gibbs Sampling<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>The Gibbs Sampling algorithm is an approach to constructing a Markov chain where the probability of the next sample is calculated as the conditional probability given the prior sample.</p>
<p>Given a target density <span class="math notranslate nohighlight">\(\pi(x_1, \cdots, x_d)\)</span> we sample through sampling from <span class="math notranslate nohighlight">\(\pi(x_i | x{-i})\)</span> to update the <span class="math notranslate nohighlight">\(i^{th}\)</span> component.</p>
<p>If our current state is <span class="math notranslate nohighlight">\((x_n, y_n, z_n)\)</span> at the <span class="math notranslate nohighlight">\(n^{th}\)</span> iteration, then we update our parameter values withthe following steps:</p>
<ol class="simple">
<li><p>Sample <span class="math notranslate nohighlight">\(x_{n+1} \sim \pi(x | y_n, z_n)\)</span></p></li>
<li><p>Sample <span class="math notranslate nohighlight">\(y_{n+1} \sim \pi(y | x_{n+1}, z_n)\)</span></p></li>
<li><p>Sample <span class="math notranslate nohighlight">\(z_{n+1} \sim \pi(x | x_{n+1}, y_{n+1})\)</span></p></li>
</ol>
<p>At each step of the sampling we use the most recent values of all the other components in the full conditional distribution. <span id="id3">[<a class="reference internal" href="#id7" title="Donald Geman Stuart Geman. Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1984. URL: http://image.diku.dk/imagecanon/material/GemanPAMI84.pdf.">SG84</a>]</span> showed that if  <span class="math notranslate nohighlight">\(x\)</span>  is the density of our parameters at the <span class="math notranslate nohighlight">\(n^{th}\)</span> iteration, as <span class="math notranslate nohighlight">\(n \rightarrow \infty\)</span> then <span class="math notranslate nohighlight">\(p(x_n, y_n, z_n) \rightarrow p(x,y,z)\)</span>.</p>
<div class="section" id="example-gibbs-sampler-for-unknown-mu-and-sigma">
<h3>Example: Gibbs Sampler for unknown <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span><a class="headerlink" href="#example-gibbs-sampler-for-unknown-mu-and-sigma" title="Permalink to this headline">¶</a></h3>
<p>First we start by recalling that a gaussian mixture model has the following form:</p>
<div class="math notranslate nohighlight">
\[
p(x|\theta) = \sum_i \pi_i \phi_{\theta_i}
\]</div>
<p>where,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\phi_{\theta_i}(x) &amp; \sim N(\mu_i, \sigma^2_i) \\
\pi_i &amp; = \text{weight/proportion of $i^{th}$ normal}
\end{align*}
\end{split}\]</div>
<p>We can now define our prior distributions. We’ll use conjugate priors because they allow us to easily compute posterior distributions. We should also point out that the choice of prior hyper parameters can make our calculations easier as well. We define our priors over <span class="math notranslate nohighlight">\(\mu_j,\sigma^2_j,\pi\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(\pi) &amp; \sim Dir(\alpha)\\
p(\mu_j) &amp; \sim N(\mu_0 = 0, \tau^2 = 1)\\
p(\sigma_j^2) &amp; \sim IG(\delta = 1, \psi = 1)
\end{align*}
\end{split}\]</div>
<div class="section" id="full-conditional-for-mu">
<h4>Full Conditional for <span class="math notranslate nohighlight">\(\mu\)</span>:<a class="headerlink" href="#full-conditional-for-mu" title="Permalink to this headline">¶</a></h4>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(\mu|x, z) &amp; = \int_0^{\infty}\int_0^{\infty}p(\theta|x,z)d\pi d\sigma\\
&amp; \propto \prod_{n=1}^N\prod_{j=1}^K\phi_{\theta_j}(x_i)^{z_j}\prod_{j=1}^K\exp\left[-\frac{\mu_j^2}
{2}\right]\\
\end{align*}
\end{split}\]</div>
<p>We can stick with a singular instance of <span class="math notranslate nohighlight">\(\mu\)</span> to simplify this a bit and get rid of the product over <span class="math notranslate nohighlight">\(K\)</span> because we know that the calculation is going to be the same for all <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
&amp; \propto \prod_{n=1}^N\phi_{\theta_1}(x_i)^{z_1}\exp\left[-\frac{\mu_1^2}{2}\right]\\
&amp; \propto \exp\left[-\frac{\sum_{i=1}^Nz_{i1}(x_i - \mu_1)^2}{2\sigma_j^2} - \frac{\mu_1^2}{2}\right]\\
&amp; \propto \exp\left[-\frac{\sum_{i=1}^Nz_{i1}x_i^2 - 2\mu_1x_iz_{i1} + z_{i1}\mu_1^2}{2\sigma_j^2} - \frac{\mu_1^2}{2}\right]\\
p(\mu | x, z) &amp; \propto \exp\left[-\frac{\sum_{i=1}^Nz_{i1}x_i^2 - 2\mu_1x_iz_{i1} + z_{i1}\mu_1^2 + \sigma^2_j\mu_j^2}{2\sigma_j^2}\right]
\end{align*}
\end{split}\]</div>
<p>Now let <span class="math notranslate nohighlight">\(\sum_{i=1}^Nz_{ij}x_i=\tilde{x_j}\)</span> and <span class="math notranslate nohighlight">\(\sum_{i=1}^Nz_{ij}=n_j\)</span>. We can also see that the first
term <span class="math notranslate nohighlight">\(\sum_{i=1}^Nz_{i1}x_i^2\)</span> does not depend on <span class="math notranslate nohighlight">\(\mu_j\)</span> so this can be factored out and absorbed into the constant term. We’re going to need to complete the square here to isolate our normal parameters.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(\mu | x, z) &amp; \propto \exp\left[-\frac{2\tilde{x_j}\mu_j + (n_j + \sigma^2_j)\mu_j^2}{2\sigma_j^2}\right]\\
&amp; \propto \exp\left[-(n_j + \sigma^2_j)\frac{\mu_j^2 + 2\left(\frac{\tilde{x_j}}{n_j + \sigma^2_j}\right)\mu_j - \left(\frac{\tilde{x_j}}{n_j + \sigma^2_j}\right)^2 + \left(\frac{\tilde{x_j}}{n_j + \sigma^2_j}\right)^2}{2\sigma_j^2}\right]\\
&amp; \propto \exp\left[-(n_j + \sigma^2_j)\frac{\left(\mu_j - \frac{\tilde{x_j}}{n_j + \sigma^2_j}\right)^2}{2\sigma_j^2}\right]\\
p(\mu | x, z) &amp; \sim N\left(\frac{\tilde{x_j}}{n_j + \sigma^2_j}, \frac{\sigma^2_j}{n_j + \sigma^2_j}\right)
\end{align*}
\end{split}\]</div>
<p>Note that if we use the prior
$<span class="math notranslate nohighlight">\(p(\mu_j|\mu_0,\tau^2) = N(0, \sigma^2_j)\)</span>$ we get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(\mu | x, z) \sim N \left(\frac{\tilde{x_j}}{n_j + 1}, \frac{\sigma^2_j}{n_j + 1}\right)\\
\end{align*}
\end{split}\]</div>
</div>
<div class="section" id="full-conditional-for-sigma-2">
<h4>Full Conditional for <span class="math notranslate nohighlight">\(\sigma^2\)</span>:<a class="headerlink" href="#full-conditional-for-sigma-2" title="Permalink to this headline">¶</a></h4>
<p>Moving on to <span class="math notranslate nohighlight">\(\sigma\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(\sigma^2|x, z) &amp; = \int_0^{\infty}\int_0^{\infty}p(\theta|x,z)d\pi d\mu\\
&amp; \propto \prod_{n=1}^N\prod_{j=1}^K\phi_{\theta_j}(x_i)^{z_j}\prod_{j=1}^K \left(\sigma^2_j\right)^{-2}\exp\left[-\frac{1}{\sigma^2_j}\right]
\end{align*}
\end{split}\]</div>
<p>Again we can isolate to <span class="math notranslate nohighlight">\(j=1\)</span> knowing that it’s the same for all <span class="math notranslate nohighlight">\(j\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
&amp; \propto \prod_{n=1}^N\phi_{\theta_j}(x_i)^{z_j}\left(\sigma^2_j\right)^{-2}\exp\left[-\frac{1}{\sigma^2_j}\right]\\
&amp; \propto \left(\sigma^2_j\right)^{-\frac{\left(\sum_{i=1}^Nz_{i,j}\right) -2 -2}{2}}\exp\left[-\frac{1}{\sigma^2_j}- \frac{\sum_{i=1}^N(x-\mu_j)^2}{2\sigma^2_j}\right]\\
&amp; \propto \left(\sigma^2_j\right)^{-\left(\frac{1}{2}n_j + 1\right) - 1}\exp\left[\frac{1 + \frac{1}{2}\sum_{i=1}^N(x-\mu_j)^2}{\sigma^2_j}\right]\\
&amp; \sim IG\left(\frac{1}{2}n_j + 1, 1 + \frac{1}{2}\sum_{i=1}^N(x-\mu_j)^2\right)
\end{align*}
\end{split}\]</div>
<div class="section" id="full-conditional-for-pi">
<h5>Full conditional for <span class="math notranslate nohighlight">\(\pi\)</span>:<a class="headerlink" href="#full-conditional-for-pi" title="Permalink to this headline">¶</a></h5>
<!-- $$
\begin{align*}
p(\pmb{\pi}) &amp; \sim Dir(\pmb{\alpha})\\
p(\mu_j) &amp; \sim N(\mu_0 = 0, \tau^2 = 1)\\
p(\sigma_j^2) &amp; \sim IG(\delta = 1, \psi = 1)
\end{align*}
$$ -->
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
% p(\pi|x, z, \pmb{\sigma}, \pmb{\mu}) &amp;amp; \propto \prod_{j=1}^K \pi_j^{\alpha_j - 1 + \sum_{i=1}^N z_i}\\
p(\pi|x, z) &amp; \sim Dir\left(\sum_{i=1}^Nz_1 + \alpha_1, ..., \sum_{i=1}^Nz_k + \alpha_k\right) 
\end{align*}\
\end{split}\]</div>
</div>
</div>
</div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<div class="docutils container" id="id4">
<dl class="citation">
<dt class="label" id="id5"><span class="brackets"><a class="fn-backref" href="#id1">Bis06</a></span></dt>
<dd><p>Christopher Bishop. <em>Pattern Recognition and Machine Learning</em>. Springer, January 2006. URL: <a class="reference external" href="https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/">https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/</a>.</p>
</dd>
<dt class="label" id="id7"><span class="brackets"><a class="fn-backref" href="#id3">SG84</a></span></dt>
<dd><p>Donald Geman Stuart Geman. <em>Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1984. URL: <a class="reference external" href="http://image.diku.dk/imagecanon/material/GemanPAMI84.pdf">http://image.diku.dk/imagecanon/material/GemanPAMI84.pdf</a>.</p>
</dd>
</dl>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "TheReconPilot/gaussian-mixture-models",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Gibbs-Sampling"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="../EM/Experiments.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Experiments</p>
        </div>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Goirik Chakrabarty, Purva Parmar<br/>
    
        &copy; Copyright 2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>